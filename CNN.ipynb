{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_excel(\"Data/tr.xlsx\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_missing_sample = []\n",
    "row_value_count = train_data.apply(pd.Series.value_counts,axis=1,dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "#Data transform\n",
    "train_y = pd.DataFrame(train_data['outcome']) \n",
    "train_X = pd.DataFrame(train_data.drop(['outcome'],axis=1))\n",
    "\n",
    "train_X = tools.data_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>LOS</th>\n",
       "      <th>Joint</th>\n",
       "      <th>Drain</th>\n",
       "      <th>Cemented</th>\n",
       "      <th>Commercial_ALBC</th>\n",
       "      <th>Non_commercial_ALBC</th>\n",
       "      <th>cci_index</th>\n",
       "      <th>elx_index</th>\n",
       "      <th>Blood_trans</th>\n",
       "      <th>ASA</th>\n",
       "      <th>CBC_WBC</th>\n",
       "      <th>CBC_RBC</th>\n",
       "      <th>CBC_HG</th>\n",
       "      <th>CBC_HT</th>\n",
       "      <th>CBC_MCV</th>\n",
       "      <th>CBC_MCH</th>\n",
       "      <th>CBC_MCHC</th>\n",
       "      <th>CBC_RDW</th>\n",
       "      <th>CBC_Platelet</th>\n",
       "      <th>CBC_RDWCV</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Crea</th>\n",
       "      <th>GOT</th>\n",
       "      <th>GPT</th>\n",
       "      <th>ALB</th>\n",
       "      <th>Na</th>\n",
       "      <th>K</th>\n",
       "      <th>UA</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>Congestive Heart Failure</th>\n",
       "      <th>Cardiac Arrhythmia</th>\n",
       "      <th>Valvular Disease</th>\n",
       "      <th>Heart disease</th>\n",
       "      <th>Pulmonary Circulation Disorders</th>\n",
       "      <th>Peripheral Vascular Disorders</th>\n",
       "      <th>Hypertension Uncomplicated</th>\n",
       "      <th>Paralysis</th>\n",
       "      <th>Other Neurological Disorders</th>\n",
       "      <th>Chronic Pulmonary Disease</th>\n",
       "      <th>Lung disease</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Hypothyroidism</th>\n",
       "      <th>Renal Failure</th>\n",
       "      <th>Liver Disease</th>\n",
       "      <th>Peptic Ulcer Disease excluding bleeding</th>\n",
       "      <th>AIDS/HIV</th>\n",
       "      <th>Lymphoma</th>\n",
       "      <th>Metastatic Cancer</th>\n",
       "      <th>Solid Tumor without Metastasis</th>\n",
       "      <th>Cancer history</th>\n",
       "      <th>Rheumatoid Arthritis/collagen</th>\n",
       "      <th>Coagulopathy</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Weight Loss</th>\n",
       "      <th>Fluid and Electrolyte Disorders</th>\n",
       "      <th>Blood Loss Anemia</th>\n",
       "      <th>Deficiency Anemia</th>\n",
       "      <th>Anemia</th>\n",
       "      <th>Alcohol Abuse</th>\n",
       "      <th>Drug Abuse</th>\n",
       "      <th>Psychoses</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Psyciatric disorder</th>\n",
       "      <th>OP_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>52065.000000</td>\n",
       "      <td>52065.000000</td>\n",
       "      <td>52065.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>32001.000000</td>\n",
       "      <td>16408.000000</td>\n",
       "      <td>16380.000000</td>\n",
       "      <td>24734.000000</td>\n",
       "      <td>24617.000000</td>\n",
       "      <td>16359.000000</td>\n",
       "      <td>16372.000000</td>\n",
       "      <td>16369.000000</td>\n",
       "      <td>16364.000000</td>\n",
       "      <td>16554.000000</td>\n",
       "      <td>3595.000000</td>\n",
       "      <td>24102.000000</td>\n",
       "      <td>22129.000000</td>\n",
       "      <td>22877.000000</td>\n",
       "      <td>17847.000000</td>\n",
       "      <td>15268.000000</td>\n",
       "      <td>15894.000000</td>\n",
       "      <td>16166.000000</td>\n",
       "      <td>12717.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>52159.000000</td>\n",
       "      <td>32053.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.303620</td>\n",
       "      <td>0.331874</td>\n",
       "      <td>5.619456</td>\n",
       "      <td>1.328726</td>\n",
       "      <td>0.739086</td>\n",
       "      <td>0.671524</td>\n",
       "      <td>0.092103</td>\n",
       "      <td>0.583178</td>\n",
       "      <td>0.701490</td>\n",
       "      <td>0.938975</td>\n",
       "      <td>0.141433</td>\n",
       "      <td>2.414425</td>\n",
       "      <td>8.233603</td>\n",
       "      <td>5.657630</td>\n",
       "      <td>13.687125</td>\n",
       "      <td>38.873498</td>\n",
       "      <td>90.645497</td>\n",
       "      <td>30.863361</td>\n",
       "      <td>34.343660</td>\n",
       "      <td>21.280638</td>\n",
       "      <td>241.175178</td>\n",
       "      <td>14.694887</td>\n",
       "      <td>18.611407</td>\n",
       "      <td>1.761761</td>\n",
       "      <td>30.133059</td>\n",
       "      <td>27.044994</td>\n",
       "      <td>5.027797</td>\n",
       "      <td>141.864696</td>\n",
       "      <td>4.971970</td>\n",
       "      <td>6.767256</td>\n",
       "      <td>0.396307</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>0.035679</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.081443</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>0.274929</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.057056</td>\n",
       "      <td>0.059683</td>\n",
       "      <td>0.119078</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.066470</td>\n",
       "      <td>0.055887</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.039686</td>\n",
       "      <td>0.045112</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.011829</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.015472</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>0.011848</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.021166</td>\n",
       "      <td>255.064175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.214685</td>\n",
       "      <td>0.470890</td>\n",
       "      <td>2.891653</td>\n",
       "      <td>0.469755</td>\n",
       "      <td>0.439137</td>\n",
       "      <td>0.469664</td>\n",
       "      <td>0.289174</td>\n",
       "      <td>0.493038</td>\n",
       "      <td>1.345236</td>\n",
       "      <td>1.429831</td>\n",
       "      <td>0.348471</td>\n",
       "      <td>0.545605</td>\n",
       "      <td>2.405581</td>\n",
       "      <td>0.544212</td>\n",
       "      <td>1.819272</td>\n",
       "      <td>5.162323</td>\n",
       "      <td>7.014657</td>\n",
       "      <td>2.743574</td>\n",
       "      <td>1.093986</td>\n",
       "      <td>12.650694</td>\n",
       "      <td>68.958212</td>\n",
       "      <td>1.198110</td>\n",
       "      <td>7.466866</td>\n",
       "      <td>0.858865</td>\n",
       "      <td>104.882890</td>\n",
       "      <td>22.373833</td>\n",
       "      <td>0.768206</td>\n",
       "      <td>3.176008</td>\n",
       "      <td>0.742038</td>\n",
       "      <td>1.815913</td>\n",
       "      <td>0.945208</td>\n",
       "      <td>0.152703</td>\n",
       "      <td>0.185491</td>\n",
       "      <td>0.146278</td>\n",
       "      <td>0.327844</td>\n",
       "      <td>0.051183</td>\n",
       "      <td>0.079409</td>\n",
       "      <td>0.446483</td>\n",
       "      <td>0.050621</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.231952</td>\n",
       "      <td>0.244860</td>\n",
       "      <td>0.323884</td>\n",
       "      <td>0.103423</td>\n",
       "      <td>0.166206</td>\n",
       "      <td>0.249104</td>\n",
       "      <td>0.229705</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>0.042414</td>\n",
       "      <td>0.060087</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>0.224930</td>\n",
       "      <td>0.203985</td>\n",
       "      <td>0.082448</td>\n",
       "      <td>0.108118</td>\n",
       "      <td>0.043962</td>\n",
       "      <td>0.123421</td>\n",
       "      <td>0.061186</td>\n",
       "      <td>0.094796</td>\n",
       "      <td>0.115883</td>\n",
       "      <td>0.108205</td>\n",
       "      <td>0.033897</td>\n",
       "      <td>0.055301</td>\n",
       "      <td>0.133309</td>\n",
       "      <td>0.151594</td>\n",
       "      <td>81.919669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>48.110000</td>\n",
       "      <td>12.420000</td>\n",
       "      <td>25.130000</td>\n",
       "      <td>12.340000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.870000</td>\n",
       "      <td>5.320000</td>\n",
       "      <td>12.490000</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>88.610000</td>\n",
       "      <td>30.120000</td>\n",
       "      <td>33.730000</td>\n",
       "      <td>14.040000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>13.960000</td>\n",
       "      <td>14.270000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>140.320000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>5.540000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.970000</td>\n",
       "      <td>5.630000</td>\n",
       "      <td>13.790000</td>\n",
       "      <td>39.300000</td>\n",
       "      <td>91.610000</td>\n",
       "      <td>31.220000</td>\n",
       "      <td>34.430000</td>\n",
       "      <td>14.740000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>14.460000</td>\n",
       "      <td>17.270000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>5.210000</td>\n",
       "      <td>142.320000</td>\n",
       "      <td>5.160000</td>\n",
       "      <td>6.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>248.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.270000</td>\n",
       "      <td>5.960000</td>\n",
       "      <td>14.890000</td>\n",
       "      <td>42.400000</td>\n",
       "      <td>94.510000</td>\n",
       "      <td>32.320000</td>\n",
       "      <td>35.030000</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>15.060000</td>\n",
       "      <td>21.270000</td>\n",
       "      <td>2.180000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>143.320000</td>\n",
       "      <td>5.530000</td>\n",
       "      <td>7.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>302.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>178.770000</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>20.590000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>124.110000</td>\n",
       "      <td>42.720000</td>\n",
       "      <td>40.230000</td>\n",
       "      <td>67.940000</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>26.760000</td>\n",
       "      <td>140.670000</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>15643.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>7.010000</td>\n",
       "      <td>169.320000</td>\n",
       "      <td>8.130000</td>\n",
       "      <td>18.740000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3078.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AGE           SEX           LOS         Joint         Drain  \\\n",
       "count  52065.000000  52065.000000  52065.000000  52159.000000  52159.000000   \n",
       "mean      65.303620      0.331874      5.619456      1.328726      0.739086   \n",
       "std       12.214685      0.470890      2.891653      0.469755      0.439137   \n",
       "min       12.000000      0.000000      1.000000      1.000000      0.000000   \n",
       "25%       59.000000      0.000000      4.000000      1.000000      0.000000   \n",
       "50%       68.000000      0.000000      5.000000      1.000000      1.000000   \n",
       "75%       74.000000      1.000000      7.000000      2.000000      1.000000   \n",
       "max       99.000000      1.000000     72.000000      2.000000      1.000000   \n",
       "\n",
       "           Cemented  Commercial_ALBC  Non_commercial_ALBC     cci_index  \\\n",
       "count  52159.000000     52159.000000         52159.000000  52159.000000   \n",
       "mean       0.671524         0.092103             0.583178      0.701490   \n",
       "std        0.469664         0.289174             0.493038      1.345236   \n",
       "min        0.000000         0.000000             0.000000      0.000000   \n",
       "25%        0.000000         0.000000             0.000000      0.000000   \n",
       "50%        1.000000         0.000000             1.000000      0.000000   \n",
       "75%        1.000000         0.000000             1.000000      1.000000   \n",
       "max        1.000000         1.000000             1.000000     15.000000   \n",
       "\n",
       "          elx_index   Blood_trans           ASA       CBC_WBC       CBC_RBC  \\\n",
       "count  52159.000000  52159.000000  32001.000000  16408.000000  16380.000000   \n",
       "mean       0.938975      0.141433      2.414425      8.233603      5.657630   \n",
       "std        1.429831      0.348471      0.545605      2.405581      0.544212   \n",
       "min        0.000000      0.000000      1.000000      2.170000      3.060000   \n",
       "25%        0.000000      0.000000      2.000000      6.870000      5.320000   \n",
       "50%        0.000000      0.000000      2.000000      7.970000      5.630000   \n",
       "75%        2.000000      0.000000      3.000000      9.270000      5.960000   \n",
       "max       13.000000      1.000000      4.000000    178.770000      9.250000   \n",
       "\n",
       "             CBC_HG        CBC_HT       CBC_MCV       CBC_MCH      CBC_MCHC  \\\n",
       "count  24734.000000  24617.000000  16359.000000  16372.000000  16369.000000   \n",
       "mean      13.687125     38.873498     90.645497     30.863361     34.343660   \n",
       "std        1.819272      5.162323      7.014657      2.743574      1.093986   \n",
       "min        1.190000     19.800000     48.110000     12.420000     25.130000   \n",
       "25%       12.490000     35.400000     88.610000     30.120000     33.730000   \n",
       "50%       13.790000     39.300000     91.610000     31.220000     34.430000   \n",
       "75%       14.890000     42.400000     94.510000     32.320000     35.030000   \n",
       "max       20.590000     57.000000    124.110000     42.720000     40.230000   \n",
       "\n",
       "            CBC_RDW  CBC_Platelet    CBC_RDWCV           BUN          Crea  \\\n",
       "count  16364.000000  16554.000000  3595.000000  24102.000000  22129.000000   \n",
       "mean      21.280638    241.175178    14.694887     18.611407      1.761761   \n",
       "std       12.650694     68.958212     1.198110      7.466866      0.858865   \n",
       "min       12.340000     16.100000    12.500000      1.800000      0.280000   \n",
       "25%       14.040000    199.000000    13.960000     14.270000      1.030000   \n",
       "50%       14.740000    236.000000    14.460000     17.270000      1.940000   \n",
       "75%       17.240000    278.000000    15.060000     21.270000      2.180000   \n",
       "max       67.940000    992.000000    26.760000    140.670000     16.490000   \n",
       "\n",
       "                GOT           GPT           ALB            Na             K  \\\n",
       "count  22877.000000  17847.000000  15268.000000  15894.000000  16166.000000   \n",
       "mean      30.133059     27.044994      5.027797    141.864696      4.971970   \n",
       "std      104.882890     22.373833      0.768206      3.176008      0.742038   \n",
       "min        4.000000      2.000000      2.000000     22.100000      2.300000   \n",
       "25%       21.000000     16.000000      4.350000    140.320000      4.300000   \n",
       "50%       25.000000     21.000000      5.210000    142.320000      5.160000   \n",
       "75%       31.000000     30.000000      5.710000    143.320000      5.530000   \n",
       "max    15643.000000    506.000000      7.010000    169.320000      8.130000   \n",
       "\n",
       "                 UA     Diagnosis  Congestive Heart Failure  \\\n",
       "count  12717.000000  52159.000000              52159.000000   \n",
       "mean       6.767256      0.396307                  0.023888   \n",
       "std        1.815913      0.945208                  0.152703   \n",
       "min        1.100000      0.000000                  0.000000   \n",
       "25%        5.540000      0.000000                  0.000000   \n",
       "50%        6.640000      0.000000                  0.000000   \n",
       "75%        7.840000      0.000000                  0.000000   \n",
       "max       18.740000      5.000000                  1.000000   \n",
       "\n",
       "       Cardiac Arrhythmia  Valvular Disease  Heart disease  \\\n",
       "count        52159.000000      52159.000000   52159.000000   \n",
       "mean             0.035679          0.021875       0.081443   \n",
       "std              0.185491          0.146278       0.327844   \n",
       "min              0.000000          0.000000       0.000000   \n",
       "25%              0.000000          0.000000       0.000000   \n",
       "50%              0.000000          0.000000       0.000000   \n",
       "75%              0.000000          0.000000       0.000000   \n",
       "max              1.000000          1.000000       3.000000   \n",
       "\n",
       "       Pulmonary Circulation Disorders  Peripheral Vascular Disorders  \\\n",
       "count                     52159.000000                   52159.000000   \n",
       "mean                          0.002627                       0.006346   \n",
       "std                           0.051183                       0.079409   \n",
       "min                           0.000000                       0.000000   \n",
       "25%                           0.000000                       0.000000   \n",
       "50%                           0.000000                       0.000000   \n",
       "75%                           0.000000                       0.000000   \n",
       "max                           1.000000                       1.000000   \n",
       "\n",
       "       Hypertension Uncomplicated     Paralysis  Other Neurological Disorders  \\\n",
       "count                52159.000000  52159.000000                  52159.000000   \n",
       "mean                     0.274929      0.002569                      0.014283   \n",
       "std                      0.446483      0.050621                      0.118657   \n",
       "min                      0.000000      0.000000                      0.000000   \n",
       "25%                      0.000000      0.000000                      0.000000   \n",
       "50%                      0.000000      0.000000                      0.000000   \n",
       "75%                      1.000000      0.000000                      0.000000   \n",
       "max                      1.000000      1.000000                      1.000000   \n",
       "\n",
       "       Chronic Pulmonary Disease  Lung disease      Diabetes  Hypothyroidism  \\\n",
       "count               52159.000000  52159.000000  52159.000000    52159.000000   \n",
       "mean                    0.057056      0.059683      0.119078        0.010813   \n",
       "std                     0.231952      0.244860      0.323884        0.103423   \n",
       "min                     0.000000      0.000000      0.000000        0.000000   \n",
       "25%                     0.000000      0.000000      0.000000        0.000000   \n",
       "50%                     0.000000      0.000000      0.000000        0.000000   \n",
       "75%                     0.000000      0.000000      0.000000        0.000000   \n",
       "max                     1.000000      2.000000      1.000000        1.000000   \n",
       "\n",
       "       Renal Failure  Liver Disease  Peptic Ulcer Disease excluding bleeding  \\\n",
       "count   52159.000000   52159.000000                             52159.000000   \n",
       "mean        0.028432       0.066470                                 0.055887   \n",
       "std         0.166206       0.249104                                 0.229705   \n",
       "min         0.000000       0.000000                                 0.000000   \n",
       "25%         0.000000       0.000000                                 0.000000   \n",
       "50%         0.000000       0.000000                                 0.000000   \n",
       "75%         0.000000       0.000000                                 0.000000   \n",
       "max         1.000000       1.000000                                 1.000000   \n",
       "\n",
       "           AIDS/HIV      Lymphoma  Metastatic Cancer  \\\n",
       "count  52159.000000  52159.000000       52159.000000   \n",
       "mean       0.000403      0.001802           0.003624   \n",
       "std        0.020061      0.042414           0.060087   \n",
       "min        0.000000      0.000000           0.000000   \n",
       "25%        0.000000      0.000000           0.000000   \n",
       "50%        0.000000      0.000000           0.000000   \n",
       "75%        0.000000      0.000000           0.000000   \n",
       "max        1.000000      1.000000           1.000000   \n",
       "\n",
       "       Solid Tumor without Metastasis  Cancer history  \\\n",
       "count                    52159.000000    52159.000000   \n",
       "mean                         0.039686        0.045112   \n",
       "std                          0.195223        0.224930   \n",
       "min                          0.000000        0.000000   \n",
       "25%                          0.000000        0.000000   \n",
       "50%                          0.000000        0.000000   \n",
       "75%                          0.000000        0.000000   \n",
       "max                          1.000000        3.000000   \n",
       "\n",
       "       Rheumatoid Arthritis/collagen  Coagulopathy       Obesity  \\\n",
       "count                   52159.000000  52159.000000  52159.000000   \n",
       "mean                        0.043502      0.006844      0.011829   \n",
       "std                         0.203985      0.082448      0.108118   \n",
       "min                         0.000000      0.000000      0.000000   \n",
       "25%                         0.000000      0.000000      0.000000   \n",
       "50%                         0.000000      0.000000      0.000000   \n",
       "75%                         0.000000      0.000000      0.000000   \n",
       "max                         1.000000      1.000000      1.000000   \n",
       "\n",
       "        Weight Loss  Fluid and Electrolyte Disorders  Blood Loss Anemia  \\\n",
       "count  52159.000000                     52159.000000       52159.000000   \n",
       "mean       0.001936                         0.015472           0.003758   \n",
       "std        0.043962                         0.123421           0.061186   \n",
       "min        0.000000                         0.000000           0.000000   \n",
       "25%        0.000000                         0.000000           0.000000   \n",
       "50%        0.000000                         0.000000           0.000000   \n",
       "75%        0.000000                         0.000000           0.000000   \n",
       "max        1.000000                         1.000000           1.000000   \n",
       "\n",
       "       Deficiency Anemia        Anemia  Alcohol Abuse    Drug Abuse  \\\n",
       "count       52159.000000  52159.000000   52159.000000  52159.000000   \n",
       "mean            0.009068      0.012826       0.011848      0.001150   \n",
       "std             0.094796      0.115883       0.108205      0.033897   \n",
       "min             0.000000      0.000000       0.000000      0.000000   \n",
       "25%             0.000000      0.000000       0.000000      0.000000   \n",
       "50%             0.000000      0.000000       0.000000      0.000000   \n",
       "75%             0.000000      0.000000       0.000000      0.000000   \n",
       "max             1.000000      2.000000       1.000000      1.000000   \n",
       "\n",
       "          Psychoses    Depression  Psyciatric disorder       OP_time  \n",
       "count  52159.000000  52159.000000         52159.000000  32053.000000  \n",
       "mean       0.003068      0.018099             0.021166    255.064175  \n",
       "std        0.055301      0.133309             0.151594     81.919669  \n",
       "min        0.000000      0.000000             0.000000      4.000000  \n",
       "25%        0.000000      0.000000             0.000000    198.000000  \n",
       "50%        0.000000      0.000000             0.000000    248.000000  \n",
       "75%        0.000000      0.000000             0.000000    302.000000  \n",
       "max        1.000000      1.000000             2.000000   3078.000000  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tools\n",
    "tools.set_pandas_display_options()\n",
    "train_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[879, 51280]\n",
      "[879, 27076]\n"
     ]
    }
   ],
   "source": [
    "threshhold =len(train_X.columns)*0.25\n",
    "over_missing = row_value_count[np.nan]<=threshhold\n",
    "class_0 = train_y['outcome']==1\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "train_X = train_X[over_missing|class_0]\n",
    "train_y = train_y[over_missing|class_0]\n",
    "print(sorted(train_y.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718, 21646]\n",
      "[161, 5430]\n"
     ]
    }
   ],
   "source": [
    "# Data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    train_X ,\n",
    "    train_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "print(sorted(val_y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718, 21646]\n",
      "[718, 7180]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler as RUS\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "\n",
    "rus = RUS(sampling_strategy=0.1,random_state=42)\n",
    "train_X,train_y = rus.fit_resample(train_X,train_y)\n",
    "\n",
    "print(sorted(train_y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filled continuous missing value with median\n",
      "filled nominal missing value with  constant\n"
     ]
    }
   ],
   "source": [
    "# Missing value imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import tools\n",
    "feature_kind = tools.init_feature_kind(train_X)\n",
    "cont,cate = tools.get_feature_kind(train_X,feature_kind)  \n",
    "\n",
    "strategy = 'median'\n",
    "\n",
    "imp_mean = IterativeImputer(max_iter=100,random_state=0)\n",
    "imp_mean.fit(train_X[cont])\n",
    "\n",
    "train_X[cont] = imp_mean.transform(train_X[cont])\n",
    "val_X[cont] = imp_mean.transform(val_X[cont])\n",
    "\n",
    "print(\"filled continuous missing value with \"+strategy)\n",
    "\n",
    "strategy = 'constant'\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=strategy,fill_value=10.0)\n",
    "imp.fit(train_X[cate])\n",
    "\n",
    "train_X[cate] = imp.transform(train_X[cate])\n",
    "val_X[cate] = imp.transform(val_X[cate])\n",
    "\n",
    "\n",
    "print(\"filled nominal missing value with \",strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "cont,cate = tools.get_feature_kind(train_X,feature_kind)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_X[cont])\n",
    "train_X[cont] = scaler.transform(train_X[cont])\n",
    "val_X[cont] = scaler.transform(val_X[cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  :\t:\n",
      "  (7873, 1)\t1.0\n",
      "  (7874, 1)\t1.0\n",
      "  (7875, 1)\t1.0\n",
      "  (7876, 1)\t1.0\n",
      "  (7877, 1)\t1.0\n",
      "  (7878, 1)\t1.0\n",
      "  (7879, 1)\t1.0\n",
      "  (7880, 1)\t1.0\n",
      "  (7881, 1)\t1.0\n",
      "  (7882, 1)\t1.0\n",
      "  (7883, 1)\t1.0\n",
      "  (7884, 1)\t1.0\n",
      "  (7885, 1)\t1.0\n",
      "  (7886, 1)\t1.0\n",
      "  (7887, 1)\t1.0\n",
      "  (7888, 1)\t1.0\n",
      "  (7889, 1)\t1.0\n",
      "  (7890, 1)\t1.0\n",
      "  (7891, 1)\t1.0\n",
      "  (7892, 1)\t1.0\n",
      "  (7893, 1)\t1.0\n",
      "  (7894, 1)\t1.0\n",
      "  (7895, 1)\t1.0\n",
      "  (7896, 1)\t1.0\n",
      "  (7897, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "print(enc.fit_transform(train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataloader\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "tensor_train_X = torch.from_numpy(np.array(train_X)).to(device)\n",
    "tensor_train_y = torch.from_numpy(np.array(train_y)).to(device)# data type is long\n",
    "\n",
    "\n",
    "# create feature and targets tensor for val set.\n",
    "tensor_val_X = torch.from_numpy(np.array(val_X)).to(device)\n",
    "tensor_val_y = torch.from_numpy(np.array(val_y)).to(device) # data type is long\n",
    "\n",
    "\n",
    "# Pytorch train and val sets\n",
    "train = torch.utils.data.TensorDataset(tensor_train_X, tensor_train_y)\n",
    "val = torch.utils.data.TensorDataset(tensor_val_X, tensor_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,100)\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(100,40)\n",
    "        self.relu2 = nn.Sigmoid()\n",
    "        self.out = nn.Linear(input_dim,output_dim)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "       # out = self.fc1(x)\n",
    "      #  out = self.relu1(out)\n",
    "      #  out = self.fc2(out)\n",
    "      #  out = self.relu2(out)\n",
    "        out = self.out(x)\n",
    "        \n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "# Hyper parm\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 11000\n",
    "num_epochs = n_iters / (len(train_X) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size = batch_size, shuffle = False)\n",
    "model = ANNModel(train_X.shape[1],int((train_X.shape[1]+train_y.shape[1])/2),1).to(device)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss(weight=torch.tensor([0.1,1]))\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR( optimizer, \n",
    "                                                step_size = 50, \n",
    "                                                gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cross_entropy: weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_68680/60881545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#print(outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Calculate softmax and cross entropy loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Calculating gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cross_entropy: weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [2]"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# ANN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "performace = {}\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        outputs = model(Variable(X.float())) # Forward propagation\n",
    "        #print(outputs)\n",
    "        print(y.float())\n",
    "        loss = error(outputs,y.float()) # Calculate softmax and cross entropy loss\n",
    "        loss.backward() # Calculating gradients\n",
    "        optimizer.step() # Update parameters\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            predict = []\n",
    "            true = []\n",
    "            model.eval()\n",
    "            # Predict val dataset\n",
    "            for X, y in val_loader:\n",
    "                #print(X.float())\n",
    "                outputs = model(X.float()) # Forward propagation\n",
    "                for pred in outputs:\n",
    "                    if(int(pred)>0.5):\n",
    "                        predict.append(1)\n",
    "                    else:\n",
    "                        predict.append(0)\n",
    "                for label in y:\n",
    "                    true.append(int(label))\n",
    "            performace['count'] = (tools.get_performance(true,predict))\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                print(count)\n",
    "                print(performace['count'])\n",
    "                \n",
    "            model.train()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "758b741b386b57519efd53b073ac35bdb1f696dd4ad70fef9c572829f656d496"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
