{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_excel(\"Data/tr.xlsx\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_missing_sample = []\n",
    "row_value_count = train_data.apply(pd.Series.value_counts,axis=1,dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "#Data transform\n",
    "train_y = pd.DataFrame(train_data['outcome']) \n",
    "train_X = pd.DataFrame(train_data.drop(['outcome'],axis=1))\n",
    "\n",
    "train_X = tools.data_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[879, 51280]\n",
      "[879, 27076]\n"
     ]
    }
   ],
   "source": [
    "threshhold =len(train_X.columns)*0.25\n",
    "over_missing = row_value_count[np.nan]<=threshhold\n",
    "class_0 = train_y['outcome']==1\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "train_X = train_X[over_missing|class_0]\n",
    "train_y = train_y[over_missing|class_0]\n",
    "print(sorted(train_y.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718, 21646]\n",
      "[161, 5430]\n"
     ]
    }
   ],
   "source": [
    "# Data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    train_X ,\n",
    "    train_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "print(sorted(val_y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718, 21646]\n",
      "[718, 7180]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler as RUS\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "\n",
    "rus = RUS(sampling_strategy=0.1,random_state=42)\n",
    "train_X,train_y = rus.fit_resample(train_X,train_y)\n",
    "\n",
    "print(sorted(train_y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7898, 66)\n",
      "(7898, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold as VT\n",
    "\n",
    "vt= VT(0.2)\n",
    "\n",
    "vt.fit(train_X)\n",
    "print(train_X.shape)\n",
    "\n",
    "train_X = pd.DataFrame(vt.transform(train_X)) \n",
    "val_X =  pd.DataFrame(vt.transform(val_X))\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filled continuous missing value with median\n",
      "filled nominal missing value with  constant\n"
     ]
    }
   ],
   "source": [
    "# Missing value imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import tools\n",
    "feature_kind = tools.init_feature_kind(train_X)\n",
    "cont,cate = tools.get_feature_kind(train_X,feature_kind)  \n",
    "\n",
    "strategy = 'median'\n",
    "\n",
    "imp_mean = IterativeImputer(max_iter=50,random_state=0)\n",
    "imp_mean.fit(train_X[cont])\n",
    "\n",
    "train_X[cont] = imp_mean.transform(train_X[cont])\n",
    "val_X[cont] = imp_mean.transform(val_X[cont])\n",
    "\n",
    "print(\"filled continuous missing value with \"+strategy)\n",
    "\n",
    "strategy = 'constant'\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=strategy,fill_value=10.0)\n",
    "imp.fit(train_X[cate])\n",
    "\n",
    "train_X[cate] = imp.transform(train_X[cate])\n",
    "val_X[cate] = imp.transform(val_X[cate])\n",
    "\n",
    "\n",
    "print(\"filled nominal missing value with \",strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "cont,cate = tools.get_feature_kind(train_X,feature_kind)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_X[cont])\n",
    "train_X[cont] = scaler.transform(train_X[cont])\n",
    "val_X[cont] = scaler.transform(val_X[cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom sklearn.linear_model import RidgeCV\\n\\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(train_X, train_y)\\nimportance = np.abs(ridge.coef_)\\nfeature_names = np.array(train_X.columns)\\nplt.bar(height=importance[0], x=feature_names)\\nplt.title(\"Feature importances via coefficients\")\\nplt.show()\\n\\n\\ntrain_X=train_X[feature_names[importance[0]>0.004]]\\nval_X = val_X[feature_names[importance[0]>0.004]]\\nprint(train_X.shape)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(train_X, train_y)\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = np.array(train_X.columns)\n",
    "plt.bar(height=importance[0], x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "train_X=train_X[feature_names[importance[0]>0.004]]\n",
    "val_X = val_X[feature_names[importance[0]>0.004]]\n",
    "print(train_X.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "#用RFE,返回特徵選擇後的資料\n",
    "#參數estimator裡放機器學習模型\n",
    "#參數n_feature_to_select為要選擇的特徵個數\n",
    "#clf = LogisticRegression(class_weight={1:1,0:0.1},solver='liblinear')\n",
    "\n",
    "#record = tools.wrapper_approach(clf,train_X,train_y,val_X,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pandas_display_options() -> None:\n",
    "    display = pd.options.display\n",
    "    display.max_columns = 100\n",
    "    display.max_rows = 100\n",
    "    display.max_colwidth = 199\n",
    "    display.width = None\n",
    "set_pandas_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import seaborn as sns\\nimport matplotlib.pyplot as plt\\n#print(record)\\ndf = pd.DataFrame(record).T\\n#print(df)\\ndf.plot.line()\\nplt.title('wrapper approach on LR with PCA')\\nprint(pd.DataFrame(record).T)\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#print(record)\n",
    "df = pd.DataFrame(record).T\n",
    "#print(df)\n",
    "df.plot.line()\n",
    "plt.title('wrapper approach on LR with PCA')\n",
    "print(pd.DataFrame(record).T)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.2554991         nan 0.25694731 0.25537381\n",
      " 0.25537381 0.25529685 0.25537381 0.25565193        nan        nan\n",
      "        nan        nan        nan 0.25388267 0.25310557        nan\n",
      " 0.25311259 0.25340645        nan        nan 0.25455777        nan\n",
      " 0.25414437 0.2524689  0.25296575 0.25283566 0.25312649 0.25350891\n",
      "        nan        nan        nan        nan        nan 0.25388267\n",
      " 0.25310557        nan 0.25311259 0.25340645        nan        nan\n",
      " 0.24405033        nan 0.25148202 0.25580647 0.2558853  0.2526244\n",
      " 0.25580647 0.2557318         nan        nan        nan        nan\n",
      "        nan 0.25388267 0.25310557        nan 0.25311259 0.25340645\n",
      "        nan        nan 0.25333198        nan 0.25386078 0.25310557\n",
      " 0.25318324 0.25310557 0.25325603 0.25348456        nan        nan\n",
      "        nan        nan        nan 0.25388267 0.25310557        nan\n",
      " 0.25311259 0.25340645]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_C',\n",
       " 'param_penalty',\n",
       " 'param_solver',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression(class_weight={1:1,0:0.1},max_iter=10000) \n",
    "\n",
    "parameters = {\n",
    "    'penalty':('l1', 'l2','elasticnet', 'none'),\n",
    "    'C':[1, 10,0.1,100],\n",
    "    'solver':('newton-cg','lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "    \n",
    "    }\n",
    "\n",
    "GS = GridSearchCV(clf, parameters,n_jobs=5,scoring='f1')\n",
    "GS.fit(train_X, train_y['outcome'])\n",
    "\n",
    "\n",
    "sorted(GS.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
      "0        0.002802  3.991131e-04         0.000000        0.000000       1   \n",
      "1        0.002601  4.911295e-04         0.000000        0.000000       1   \n",
      "2        0.264265  9.931706e-02         0.003797        0.000742       1   \n",
      "3        0.002602  4.881408e-04         0.000000        0.000000       1   \n",
      "4        2.729284  5.714271e-01         0.003193        0.000405       1   \n",
      "5        0.102451  8.878321e-03         0.003200        0.000751       1   \n",
      "6        0.104424  1.363802e-02         0.003202        0.000746       1   \n",
      "7        0.058416  3.143673e-03         0.003197        0.000403       1   \n",
      "8        0.987027  1.925412e-01         0.003393        0.000490       1   \n",
      "9        1.828431  4.874221e-01         0.004000        0.000631       1   \n",
      "10       0.002488  7.387212e-04         0.000000        0.000000       1   \n",
      "11       0.002000  8.714517e-07         0.000000        0.000000       1   \n",
      "12       0.002300  7.488402e-04         0.000000        0.000000       1   \n",
      "13       0.001953  8.489330e-05         0.000000        0.000000       1   \n",
      "14       0.001999  2.219215e-06         0.000000        0.000000       1   \n",
      "15       0.447324  1.137910e-01         0.003192        0.000406       1   \n",
      "16       0.142943  1.567322e-02         0.003198        0.000400       1   \n",
      "17       0.002200  3.990698e-04         0.000000        0.000000       1   \n",
      "18       2.864117  1.354910e+00         0.003205        0.000393       1   \n",
      "19       3.408786  1.504225e+00         0.003594        0.000490       1   \n",
      "20       0.002199  4.016926e-04         0.000000        0.000000      10   \n",
      "21       0.002401  4.933455e-04         0.000000        0.000000      10   \n",
      "22       0.375234  6.534421e-02         0.003398        0.001016      10   \n",
      "23       0.002799  3.989477e-04         0.000000        0.000000      10   \n",
      "24       2.329615  8.092879e-01         0.003203        0.000400      10   \n",
      "25       0.113392  7.409036e-03         0.003402        0.000491      10   \n",
      "26       0.131497  2.365284e-02         0.003251        0.000382      10   \n",
      "27       0.084895  1.166537e-02         0.003402        0.000490      10   \n",
      "28       2.043406  7.775551e-01         0.003520        0.000449      10   \n",
      "29       2.621267  8.797078e-01         0.003400        0.000490      10   \n",
      "30       0.003200  4.016441e-04         0.000000        0.000000      10   \n",
      "31       0.002201  4.000433e-04         0.000000        0.000000      10   \n",
      "32       0.003000  1.542183e-06         0.000000        0.000000      10   \n",
      "33       0.003001  1.715289e-06         0.000000        0.000000      10   \n",
      "34       0.002597  4.877025e-04         0.000000        0.000000      10   \n",
      "35       0.485341  1.081065e-01         0.003601        0.000491      10   \n",
      "36       0.161673  1.850275e-02         0.003001        0.000002      10   \n",
      "37       0.002850  4.351286e-04         0.000000        0.000000      10   \n",
      "38       2.863811  1.323260e+00         0.003601        0.000491      10   \n",
      "39       3.305772  1.443127e+00         0.003601        0.001020      10   \n",
      "40       0.001801  4.000921e-04         0.000000        0.000000     0.1   \n",
      "41       0.002280  3.914888e-04         0.000000        0.000000     0.1   \n",
      "42       0.028217  4.441465e-03         0.002800        0.000400     0.1   \n",
      "43       0.002602  4.895552e-04         0.000000        0.000000     0.1   \n",
      "44       0.653213  5.975200e-02         0.003004        0.000629     0.1   \n",
      "45       0.085178  8.245878e-03         0.003399        0.000490     0.1   \n",
      "46       0.058877  3.732433e-03         0.003001        0.000630     0.1   \n",
      "47       0.036399  1.836555e-03         0.003198        0.000401     0.1   \n",
      "48       0.684486  4.085384e-02         0.003196        0.000403     0.1   \n",
      "49       1.381620  8.001626e-02         0.003799        0.000400     0.1   \n",
      "50       0.002533  7.761365e-04         0.000000        0.000000     0.1   \n",
      "51       0.002000  5.352484e-07         0.000000        0.000000     0.1   \n",
      "52       0.002210  3.953023e-04         0.000000        0.000000     0.1   \n",
      "53       0.002308  4.010454e-04         0.000000        0.000000     0.1   \n",
      "54       0.002800  4.007818e-04         0.000000        0.000000     0.1   \n",
      "55       0.436779  1.192874e-01         0.003406        0.000492     0.1   \n",
      "56       0.149470  1.814639e-02         0.003599        0.000490     0.1   \n",
      "57       0.002401  4.915219e-04         0.000000        0.000000     0.1   \n",
      "58       2.931620  1.416904e+00         0.003205        0.000750     0.1   \n",
      "59       3.431461  1.658450e+00         0.003800        0.000748     0.1   \n",
      "60       0.002800  3.999904e-04         0.000000        0.000000     100   \n",
      "61       0.002798  7.495654e-04         0.000000        0.000000     100   \n",
      "62       2.137477  1.031207e+00         0.003401        0.000491     100   \n",
      "63       0.003401  4.891468e-04         0.000000        0.000000     100   \n",
      "64       3.445350  1.017999e+00         0.003602        0.000491     100   \n",
      "65       0.112231  1.647427e-02         0.003000        0.000006     100   \n",
      "66       0.135236  2.104015e-02         0.002800        0.000407     100   \n",
      "67       0.062412  9.245766e-03         0.002802        0.000397     100   \n",
      "68       2.711058  1.270597e+00         0.002795        0.000398     100   \n",
      "69       3.190617  1.389873e+00         0.003599        0.000800     100   \n",
      "70       0.002417  5.092811e-04         0.000000        0.000000     100   \n",
      "71       0.002397  4.913717e-04         0.000000        0.000000     100   \n",
      "72       0.002600  4.896977e-04         0.000000        0.000000     100   \n",
      "73       0.002800  4.016640e-04         0.000000        0.000000     100   \n",
      "74       0.002200  3.994003e-04         0.000000        0.000000     100   \n",
      "75       0.427432  1.213308e-01         0.002803        0.000404     100   \n",
      "76       0.146344  1.405523e-02         0.003600        0.000489     100   \n",
      "77       0.002584  4.783740e-04         0.000000        0.000000     100   \n",
      "78       3.026749  1.468724e+00         0.003598        0.000800     100   \n",
      "79       3.153885  1.554912e+00         0.002997        0.000004     100   \n",
      "\n",
      "   param_penalty param_solver  \\\n",
      "0             l1    newton-cg   \n",
      "1             l1        lbfgs   \n",
      "2             l1    liblinear   \n",
      "3             l1          sag   \n",
      "4             l1         saga   \n",
      "5             l2    newton-cg   \n",
      "6             l2        lbfgs   \n",
      "7             l2    liblinear   \n",
      "8             l2          sag   \n",
      "9             l2         saga   \n",
      "10    elasticnet    newton-cg   \n",
      "11    elasticnet        lbfgs   \n",
      "12    elasticnet    liblinear   \n",
      "13    elasticnet          sag   \n",
      "14    elasticnet         saga   \n",
      "15          none    newton-cg   \n",
      "16          none        lbfgs   \n",
      "17          none    liblinear   \n",
      "18          none          sag   \n",
      "19          none         saga   \n",
      "20            l1    newton-cg   \n",
      "21            l1        lbfgs   \n",
      "22            l1    liblinear   \n",
      "23            l1          sag   \n",
      "24            l1         saga   \n",
      "25            l2    newton-cg   \n",
      "26            l2        lbfgs   \n",
      "27            l2    liblinear   \n",
      "28            l2          sag   \n",
      "29            l2         saga   \n",
      "30    elasticnet    newton-cg   \n",
      "31    elasticnet        lbfgs   \n",
      "32    elasticnet    liblinear   \n",
      "33    elasticnet          sag   \n",
      "34    elasticnet         saga   \n",
      "35          none    newton-cg   \n",
      "36          none        lbfgs   \n",
      "37          none    liblinear   \n",
      "38          none          sag   \n",
      "39          none         saga   \n",
      "40            l1    newton-cg   \n",
      "41            l1        lbfgs   \n",
      "42            l1    liblinear   \n",
      "43            l1          sag   \n",
      "44            l1         saga   \n",
      "45            l2    newton-cg   \n",
      "46            l2        lbfgs   \n",
      "47            l2    liblinear   \n",
      "48            l2          sag   \n",
      "49            l2         saga   \n",
      "50    elasticnet    newton-cg   \n",
      "51    elasticnet        lbfgs   \n",
      "52    elasticnet    liblinear   \n",
      "53    elasticnet          sag   \n",
      "54    elasticnet         saga   \n",
      "55          none    newton-cg   \n",
      "56          none        lbfgs   \n",
      "57          none    liblinear   \n",
      "58          none          sag   \n",
      "59          none         saga   \n",
      "60            l1    newton-cg   \n",
      "61            l1        lbfgs   \n",
      "62            l1    liblinear   \n",
      "63            l1          sag   \n",
      "64            l1         saga   \n",
      "65            l2    newton-cg   \n",
      "66            l2        lbfgs   \n",
      "67            l2    liblinear   \n",
      "68            l2          sag   \n",
      "69            l2         saga   \n",
      "70    elasticnet    newton-cg   \n",
      "71    elasticnet        lbfgs   \n",
      "72    elasticnet    liblinear   \n",
      "73    elasticnet          sag   \n",
      "74    elasticnet         saga   \n",
      "75          none    newton-cg   \n",
      "76          none        lbfgs   \n",
      "77          none    liblinear   \n",
      "78          none          sag   \n",
      "79          none         saga   \n",
      "\n",
      "                                                        params  \\\n",
      "0             {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "1                 {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "2             {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "3                   {'C': 1, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "4                  {'C': 1, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "5             {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "6                 {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "7             {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "8                   {'C': 1, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "9                  {'C': 1, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "10    {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "11        {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "12    {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "13          {'C': 1, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "14         {'C': 1, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "15          {'C': 1, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "16              {'C': 1, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "17          {'C': 1, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "18                {'C': 1, 'penalty': 'none', 'solver': 'sag'}   \n",
      "19               {'C': 1, 'penalty': 'none', 'solver': 'saga'}   \n",
      "20           {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "21               {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "22           {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "23                 {'C': 10, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "24                {'C': 10, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "25           {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "26               {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "27           {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "28                 {'C': 10, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "29                {'C': 10, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "30   {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "31       {'C': 10, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "32   {'C': 10, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "33         {'C': 10, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "34        {'C': 10, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "35         {'C': 10, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "36             {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "37         {'C': 10, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "38               {'C': 10, 'penalty': 'none', 'solver': 'sag'}   \n",
      "39              {'C': 10, 'penalty': 'none', 'solver': 'saga'}   \n",
      "40          {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "41              {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "42          {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "43                {'C': 0.1, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "44               {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "45          {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "46              {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "47          {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "48                {'C': 0.1, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "49               {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "50  {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "51      {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "52  {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "53        {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "54       {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "55        {'C': 0.1, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "56            {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "57        {'C': 0.1, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "58              {'C': 0.1, 'penalty': 'none', 'solver': 'sag'}   \n",
      "59             {'C': 0.1, 'penalty': 'none', 'solver': 'saga'}   \n",
      "60          {'C': 100, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "61              {'C': 100, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "62          {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "63                {'C': 100, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "64               {'C': 100, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "65          {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "66              {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "67          {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "68                {'C': 100, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "69               {'C': 100, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "70  {'C': 100, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "71      {'C': 100, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "72  {'C': 100, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "73        {'C': 100, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "74       {'C': 100, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "75        {'C': 100, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "76            {'C': 100, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "77        {'C': 100, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "78              {'C': 100, 'penalty': 'none', 'solver': 'sag'}   \n",
      "79             {'C': 100, 'penalty': 'none', 'solver': 'saga'}   \n",
      "\n",
      "    split0_test_score  split1_test_score  split2_test_score  \\\n",
      "0                 NaN                NaN                NaN   \n",
      "1                 NaN                NaN                NaN   \n",
      "2            0.247126           0.247788           0.268956   \n",
      "3                 NaN                NaN                NaN   \n",
      "4            0.247482           0.249633           0.274678   \n",
      "5            0.244318           0.245974           0.272597   \n",
      "6            0.244318           0.245974           0.272597   \n",
      "7            0.246110           0.248538           0.268188   \n",
      "8            0.244318           0.245974           0.272597   \n",
      "9            0.246459           0.245614           0.272206   \n",
      "10                NaN                NaN                NaN   \n",
      "11                NaN                NaN                NaN   \n",
      "12                NaN                NaN                NaN   \n",
      "13                NaN                NaN                NaN   \n",
      "14                NaN                NaN                NaN   \n",
      "15           0.249645           0.241279           0.271429   \n",
      "16           0.246459           0.242336           0.271429   \n",
      "17                NaN                NaN                NaN   \n",
      "18           0.246459           0.241983           0.271817   \n",
      "19           0.245763           0.242336           0.272206   \n",
      "20                NaN                NaN                NaN   \n",
      "21                NaN                NaN                NaN   \n",
      "22           0.245763           0.241983           0.273381   \n",
      "23                NaN                NaN                NaN   \n",
      "24           0.246110           0.242336           0.272989   \n",
      "25           0.243626           0.239766           0.272206   \n",
      "26           0.246110           0.239766           0.272206   \n",
      "27           0.243626           0.242336           0.271817   \n",
      "28           0.243281           0.242336           0.272206   \n",
      "29           0.243281           0.242336           0.273381   \n",
      "30                NaN                NaN                NaN   \n",
      "31                NaN                NaN                NaN   \n",
      "32                NaN                NaN                NaN   \n",
      "33                NaN                NaN                NaN   \n",
      "34                NaN                NaN                NaN   \n",
      "35           0.249645           0.241279           0.271429   \n",
      "36           0.246459           0.242336           0.271429   \n",
      "37                NaN                NaN                NaN   \n",
      "38           0.246459           0.241983           0.271817   \n",
      "39           0.245763           0.242336           0.272206   \n",
      "40                NaN                NaN                NaN   \n",
      "41                NaN                NaN                NaN   \n",
      "42           0.224404           0.238298           0.253731   \n",
      "43                NaN                NaN                NaN   \n",
      "44           0.231672           0.249624           0.266871   \n",
      "45           0.241379           0.250746           0.267647   \n",
      "46           0.241379           0.250746           0.268041   \n",
      "47           0.241821           0.236842           0.268222   \n",
      "48           0.241379           0.250746           0.267647   \n",
      "49           0.241379           0.250746           0.268041   \n",
      "50                NaN                NaN                NaN   \n",
      "51                NaN                NaN                NaN   \n",
      "52                NaN                NaN                NaN   \n",
      "53                NaN                NaN                NaN   \n",
      "54                NaN                NaN                NaN   \n",
      "55           0.249645           0.241279           0.271429   \n",
      "56           0.246459           0.242336           0.271429   \n",
      "57                NaN                NaN                NaN   \n",
      "58           0.246459           0.241983           0.271817   \n",
      "59           0.245763           0.242336           0.272206   \n",
      "60                NaN                NaN                NaN   \n",
      "61                NaN                NaN                NaN   \n",
      "62           0.246459           0.242690           0.272206   \n",
      "63                NaN                NaN                NaN   \n",
      "64           0.245763           0.242690           0.273775   \n",
      "65           0.246459           0.242336           0.271429   \n",
      "66           0.246459           0.242336           0.271817   \n",
      "67           0.246459           0.242336           0.271429   \n",
      "68           0.246459           0.242336           0.271817   \n",
      "69           0.245763           0.242336           0.272597   \n",
      "70                NaN                NaN                NaN   \n",
      "71                NaN                NaN                NaN   \n",
      "72                NaN                NaN                NaN   \n",
      "73                NaN                NaN                NaN   \n",
      "74                NaN                NaN                NaN   \n",
      "75           0.249645           0.241279           0.271429   \n",
      "76           0.246459           0.242336           0.271429   \n",
      "77                NaN                NaN                NaN   \n",
      "78           0.246459           0.241983           0.271817   \n",
      "79           0.245763           0.242336           0.272206   \n",
      "\n",
      "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
      "0                 NaN                NaN              NaN             NaN   \n",
      "1                 NaN                NaN              NaN             NaN   \n",
      "2            0.266854           0.246772         0.255499        0.010156   \n",
      "3                 NaN                NaN              NaN             NaN   \n",
      "4            0.267229           0.245714         0.256947        0.011742   \n",
      "5            0.266854           0.247126         0.255374        0.011891   \n",
      "6            0.266854           0.247126         0.255374        0.011891   \n",
      "7            0.267229           0.246418         0.255297        0.010173   \n",
      "8            0.266854           0.247126         0.255374        0.011891   \n",
      "9            0.266854           0.247126         0.255652        0.011467   \n",
      "10                NaN                NaN              NaN             NaN   \n",
      "11                NaN                NaN              NaN             NaN   \n",
      "12                NaN                NaN              NaN             NaN   \n",
      "13                NaN                NaN              NaN             NaN   \n",
      "14                NaN                NaN              NaN             NaN   \n",
      "15           0.263158           0.243902         0.253883        0.011578   \n",
      "16           0.262794           0.242511         0.253106        0.011849   \n",
      "17                NaN                NaN              NaN             NaN   \n",
      "18           0.262794           0.242511         0.253113        0.012034   \n",
      "19           0.263523           0.243205         0.253406        0.012172   \n",
      "20                NaN                NaN              NaN             NaN   \n",
      "21                NaN                NaN              NaN             NaN   \n",
      "22           0.267409           0.244253         0.254558        0.013124   \n",
      "23                NaN                NaN              NaN             NaN   \n",
      "24           0.265734           0.243553         0.254144        0.012693   \n",
      "25           0.263889           0.242857         0.252469        0.013053   \n",
      "26           0.263889           0.242857         0.252966        0.012751   \n",
      "27           0.263889           0.242511         0.252836        0.012523   \n",
      "28           0.264256           0.243553         0.253126        0.012593   \n",
      "29           0.264993           0.243553         0.253509        0.013079   \n",
      "30                NaN                NaN              NaN             NaN   \n",
      "31                NaN                NaN              NaN             NaN   \n",
      "32                NaN                NaN              NaN             NaN   \n",
      "33                NaN                NaN              NaN             NaN   \n",
      "34                NaN                NaN              NaN             NaN   \n",
      "35           0.263158           0.243902         0.253883        0.011578   \n",
      "36           0.262794           0.242511         0.253106        0.011849   \n",
      "37                NaN                NaN              NaN             NaN   \n",
      "38           0.262794           0.242511         0.253113        0.012034   \n",
      "39           0.263523           0.243205         0.253406        0.012172   \n",
      "40                NaN                NaN              NaN             NaN   \n",
      "41                NaN                NaN              NaN             NaN   \n",
      "42           0.263587           0.240232         0.244050        0.013484   \n",
      "43                NaN                NaN              NaN             NaN   \n",
      "44           0.272206           0.237037         0.251482        0.015943   \n",
      "45           0.273239           0.246020         0.255806        0.012439   \n",
      "46           0.273239           0.246020         0.255885        0.012515   \n",
      "47           0.269819           0.246418         0.252624        0.013735   \n",
      "48           0.273239           0.246020         0.255806        0.012439   \n",
      "49           0.272472           0.246020         0.255732        0.012304   \n",
      "50                NaN                NaN              NaN             NaN   \n",
      "51                NaN                NaN              NaN             NaN   \n",
      "52                NaN                NaN              NaN             NaN   \n",
      "53                NaN                NaN              NaN             NaN   \n",
      "54                NaN                NaN              NaN             NaN   \n",
      "55           0.263158           0.243902         0.253883        0.011578   \n",
      "56           0.262794           0.242511         0.253106        0.011849   \n",
      "57                NaN                NaN              NaN             NaN   \n",
      "58           0.262794           0.242511         0.253113        0.012034   \n",
      "59           0.263523           0.243205         0.253406        0.012172   \n",
      "60                NaN                NaN              NaN             NaN   \n",
      "61                NaN                NaN              NaN             NaN   \n",
      "62           0.262794           0.242511         0.253332        0.012028   \n",
      "63                NaN                NaN              NaN             NaN   \n",
      "64           0.263523           0.243553         0.253861        0.012542   \n",
      "65           0.262794           0.242511         0.253106        0.011849   \n",
      "66           0.262794           0.242511         0.253183        0.011970   \n",
      "67           0.262794           0.242511         0.253106        0.011849   \n",
      "68           0.263158           0.242511         0.253256        0.012029   \n",
      "69           0.263523           0.243205         0.253485        0.012293   \n",
      "70                NaN                NaN              NaN             NaN   \n",
      "71                NaN                NaN              NaN             NaN   \n",
      "72                NaN                NaN              NaN             NaN   \n",
      "73                NaN                NaN              NaN             NaN   \n",
      "74                NaN                NaN              NaN             NaN   \n",
      "75           0.263158           0.243902         0.253883        0.011578   \n",
      "76           0.262794           0.242511         0.253106        0.011849   \n",
      "77                NaN                NaN              NaN             NaN   \n",
      "78           0.262794           0.242511         0.253113        0.012034   \n",
      "79           0.263523           0.243205         0.253406        0.012172   \n",
      "\n",
      "    rank_test_score  \n",
      "0                80  \n",
      "1                55  \n",
      "2                 7  \n",
      "3                57  \n",
      "4                 1  \n",
      "5                 8  \n",
      "6                 8  \n",
      "7                11  \n",
      "8                 8  \n",
      "9                 6  \n",
      "10               48  \n",
      "11               45  \n",
      "12               60  \n",
      "13               73  \n",
      "14               72  \n",
      "15               14  \n",
      "16               33  \n",
      "17               71  \n",
      "18               29  \n",
      "19               21  \n",
      "20               61  \n",
      "21               75  \n",
      "22               12  \n",
      "23               50  \n",
      "24               13  \n",
      "25               42  \n",
      "26               39  \n",
      "27               40  \n",
      "28               28  \n",
      "29               19  \n",
      "30               62  \n",
      "31               63  \n",
      "32               64  \n",
      "33               65  \n",
      "34               66  \n",
      "35               14  \n",
      "36               33  \n",
      "37               67  \n",
      "38               29  \n",
      "39               21  \n",
      "40               68  \n",
      "41               70  \n",
      "42               44  \n",
      "43               79  \n",
      "44               43  \n",
      "45                3  \n",
      "46                2  \n",
      "47               41  \n",
      "48                3  \n",
      "49                5  \n",
      "50               74  \n",
      "51               59  \n",
      "52               76  \n",
      "53               77  \n",
      "54               78  \n",
      "55               14  \n",
      "56               33  \n",
      "57               69  \n",
      "58               29  \n",
      "59               21  \n",
      "60               49  \n",
      "61               47  \n",
      "62               25  \n",
      "63               46  \n",
      "64               18  \n",
      "65               33  \n",
      "66               27  \n",
      "67               33  \n",
      "68               26  \n",
      "69               20  \n",
      "70               58  \n",
      "71               51  \n",
      "72               52  \n",
      "73               53  \n",
      "74               54  \n",
      "75               14  \n",
      "76               33  \n",
      "77               56  \n",
      "78               29  \n",
      "79               21  \n"
     ]
    }
   ],
   "source": [
    "print(GS.best_params_)\n",
    "print(pd.DataFrame(GS.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        train                        val\n",
      "confusion matrix   [[4887, 2293], [266, 452]]  [[3671, 1759], [61, 100]]\n",
      "acc                                  0.675994                   0.674477\n",
      "precision                            0.164663                   0.053792\n",
      "f1_score                             0.261045                    0.09901\n",
      "recall                               0.629526                   0.621118\n",
      "matthews_corrcoef                    0.187249                   0.105491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "best_parameters = {\n",
    "    'C': 1, 'penalty': 'l1', 'solver': 'saga'\n",
    "}\n",
    "\n",
    "clf = LogisticRegression(**best_parameters,class_weight={1:1,0:0.1},max_iter=10000) \n",
    "\n",
    "clf.fit(train_X, train_y['outcome'])\n",
    "result = clf.predict(val_X)\n",
    "\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'train':tools.get_performance(train_y,clf.predict(train_X)),\n",
    "    'val':tools.get_performance(val_y,result)\n",
    "    }\n",
    "    ))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "758b741b386b57519efd53b073ac35bdb1f696dd4ad70fef9c572829f656d496"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
