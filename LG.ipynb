{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_excel(\"Data/tr.xlsx\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_missing_sample = []\n",
    "row_value_count = train_data.apply(pd.Series.value_counts,axis=1,dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "#Data transform\n",
    "train_y = pd.DataFrame(train_data['outcome']) \n",
    "train_X = pd.DataFrame(train_data.drop(['outcome'],axis=1))\n",
    "\n",
    "train_X = tools.data_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[879, 51280]\n",
      "[879, 27076]\n"
     ]
    }
   ],
   "source": [
    "threshhold =len(train_X.columns)*0.25\n",
    "over_missing = row_value_count[np.nan]<=threshhold\n",
    "class_0 = train_y['outcome']==1\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "train_X = train_X[over_missing|class_0]\n",
    "train_y = train_y[over_missing|class_0]\n",
    "print(sorted(train_y.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718, 21646]\n",
      "[161, 5430]\n"
     ]
    }
   ],
   "source": [
    "# Data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    train_X ,\n",
    "    train_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "print(sorted(val_y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718, 21646]\n",
      "[718, 7180]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler as RUS\n",
    "\n",
    "print(sorted(train_y.value_counts()))\n",
    "\n",
    "rus = RUS(sampling_strategy=0.1,random_state=42)\n",
    "train_X,train_y = rus.fit_resample(train_X,train_y)\n",
    "\n",
    "print(sorted(train_y.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7898, 66)\n",
      "(7898, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold as VT\n",
    "\n",
    "vt= VT(0.2)\n",
    "\n",
    "vt.fit(train_X)\n",
    "print(train_X.shape)\n",
    "\n",
    "train_X = pd.DataFrame(vt.transform(train_X)) \n",
    "val_X =  pd.DataFrame(vt.transform(val_X))\n",
    "\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filled continuous missing value with median\n",
      "filled nominal missing value with  constant\n"
     ]
    }
   ],
   "source": [
    "# Missing value imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import tools\n",
    "feature_kind = tools.init_feature_kind(train_X)\n",
    "cont,cate = tools.get_feature_kind(train_X,feature_kind)  \n",
    "\n",
    "strategy = 'median'\n",
    "\n",
    "imp_mean = IterativeImputer(max_iter=100,random_state=0)\n",
    "imp_mean.fit(train_X[cont])\n",
    "\n",
    "train_X[cont] = imp_mean.transform(train_X[cont])\n",
    "val_X[cont] = imp_mean.transform(val_X[cont])\n",
    "\n",
    "print(\"filled continuous missing value with \"+strategy)\n",
    "\n",
    "strategy = 'constant'\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=strategy,fill_value=10.0)\n",
    "imp.fit(train_X[cate])\n",
    "\n",
    "train_X[cate] = imp.transform(train_X[cate])\n",
    "val_X[cate] = imp.transform(val_X[cate])\n",
    "\n",
    "\n",
    "print(\"filled nominal missing value with \",strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "cont,cate = tools.get_feature_kind(train_X,feature_kind)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_X[cont])\n",
    "train_X[cont] = scaler.transform(train_X[cont])\n",
    "val_X[cont] = scaler.transform(val_X[cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom sklearn.linear_model import RidgeCV\\n\\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(train_X, train_y)\\nimportance = np.abs(ridge.coef_)\\nfeature_names = np.array(train_X.columns)\\nplt.bar(height=importance[0], x=feature_names)\\nplt.title(\"Feature importances via coefficients\")\\nplt.show()\\n\\n\\ntrain_X=train_X[feature_names[importance[0]>0.004]]\\nval_X = val_X[feature_names[importance[0]>0.004]]\\nprint(train_X.shape)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(train_X, train_y)\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = np.array(train_X.columns)\n",
    "plt.bar(height=importance[0], x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "train_X=train_X[feature_names[importance[0]>0.004]]\n",
    "val_X = val_X[feature_names[importance[0]>0.004]]\n",
    "print(train_X.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "#用RFE,返回特徵選擇後的資料\n",
    "#參數estimator裡放機器學習模型\n",
    "#參數n_feature_to_select為要選擇的特徵個數\n",
    "#clf = LogisticRegression(class_weight={1:1,0:0.1},solver='liblinear')\n",
    "\n",
    "#record = tools.wrapper_approach(clf,train_X,train_y,val_X,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pandas_display_options() -> None:\n",
    "    display = pd.options.display\n",
    "    display.max_columns = 100\n",
    "    display.max_rows = 100\n",
    "    display.max_colwidth = 199\n",
    "    display.width = None\n",
    "set_pandas_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import seaborn as sns\\nimport matplotlib.pyplot as plt\\n#print(record)\\ndf = pd.DataFrame(record).T\\n#print(df)\\ndf.plot.line()\\nplt.title('wrapper approach on LR with PCA')\\nprint(pd.DataFrame(record).T)\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#print(record)\n",
    "df = pd.DataFrame(record).T\n",
    "#print(df)\n",
    "df.plot.line()\n",
    "plt.title('wrapper approach on LR with PCA')\n",
    "print(pd.DataFrame(record).T)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.25514156        nan 0.25702413 0.2556649\n",
      " 0.25558656 0.25516088 0.25559662 0.25594401        nan        nan\n",
      "        nan        nan        nan 0.25395763 0.25326039        nan\n",
      " 0.25341129 0.2535508         nan        nan 0.25420303        nan\n",
      " 0.25414555 0.25239084 0.25291201 0.25290477 0.2529077  0.25322129\n",
      "        nan        nan        nan        nan        nan 0.25395763\n",
      " 0.25326039        nan 0.25341129 0.2535508         nan        nan\n",
      " 0.24352188        nan 0.25149229 0.25580647 0.25580647 0.25283309\n",
      " 0.25596238 0.25595661        nan        nan        nan        nan\n",
      "        nan 0.25395763 0.25326039        nan 0.25341129 0.2535508\n",
      "        nan        nan 0.25333849        nan 0.25330299 0.25326039\n",
      " 0.25326039 0.25326039 0.25333318 0.25312634        nan        nan\n",
      "        nan        nan        nan 0.25395763 0.25326039        nan\n",
      " 0.25341129 0.2535508 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_C',\n",
       " 'param_penalty',\n",
       " 'param_solver',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression(class_weight={1:1,0:0.1},max_iter=10000) \n",
    "\n",
    "parameters = {\n",
    "    'penalty':('l1', 'l2','elasticnet', 'none'),\n",
    "    'C':[1, 10,0.1,100],\n",
    "    'solver':('newton-cg','lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "    \n",
    "    }\n",
    "\n",
    "GS = GridSearchCV(clf, parameters,n_jobs=5,scoring='f1')\n",
    "GS.fit(train_X, train_y['outcome'])\n",
    "\n",
    "\n",
    "sorted(GS.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
      "0        0.004000  0.000000e+00         0.000000        0.000000       1   \n",
      "1        0.003601  4.895722e-04         0.000000        0.000000       1   \n",
      "2        0.417811  1.155525e-01         0.005802        0.000749       1   \n",
      "3        0.004401  4.891253e-04         0.000000        0.000000       1   \n",
      "4        4.059721  9.535429e-01         0.005199        0.000984       1   \n",
      "5        0.171772  9.490760e-03         0.005196        0.000743       1   \n",
      "6        0.184377  1.399651e-02         0.005201        0.000748       1   \n",
      "7        0.103638  2.844590e-03         0.005703        0.000986       1   \n",
      "8        1.581866  3.380774e-01         0.005307        0.001167       1   \n",
      "9        2.925200  7.140592e-01         0.006802        0.000979       1   \n",
      "10       0.004402  4.911520e-04         0.000000        0.000000       1   \n",
      "11       0.004001  1.642149e-06         0.000000        0.000000       1   \n",
      "12       0.005200  1.167972e-03         0.000000        0.000000       1   \n",
      "13       0.005001  1.095411e-03         0.000000        0.000000       1   \n",
      "14       0.004400  1.019226e-03         0.000000        0.000000       1   \n",
      "15       0.977759  2.676788e-01         0.005399        0.001017       1   \n",
      "16       0.231035  3.165454e-02         0.005000        0.001096       1   \n",
      "17       0.004201  1.469977e-03         0.000000        0.000000       1   \n",
      "18       3.973472  2.039876e+00         0.005002        0.001095       1   \n",
      "19       4.832302  2.276455e+00         0.005105        0.000488       1   \n",
      "20       0.003601  1.019498e-03         0.000000        0.000000      10   \n",
      "21       0.003602  1.021967e-03         0.000000        0.000000      10   \n",
      "22       0.563853  5.040828e-02         0.005601        0.001020      10   \n",
      "23       0.003800  3.999504e-04         0.000000        0.000000      10   \n",
      "24       3.526984  1.255136e+00         0.004602        0.000798      10   \n",
      "25       0.173553  1.010342e-02         0.004201        0.000399      10   \n",
      "26       0.228779  2.575048e-02         0.004801        0.001167      10   \n",
      "27       0.120475  2.911332e-03         0.004598        0.001358      10   \n",
      "28       2.840281  1.110940e+00         0.004801        0.001166      10   \n",
      "29       3.525385  1.189717e+00         0.004561        0.000831      10   \n",
      "30       0.003600  4.913824e-04         0.000000        0.000000      10   \n",
      "31       0.004232  1.940131e-03         0.000000        0.000000      10   \n",
      "32       0.002907  6.584613e-04         0.000000        0.000000      10   \n",
      "33       0.003801  3.989225e-04         0.000000        0.000000      10   \n",
      "34       0.003002  7.599534e-07         0.000000        0.000000      10   \n",
      "35       0.774476  2.349045e-01         0.004601        0.001200      10   \n",
      "36       0.203054  1.694375e-02         0.004601        0.000801      10   \n",
      "37       0.003601  4.899024e-04         0.000000        0.000000      10   \n",
      "38       3.615462  1.812140e+00         0.004800        0.000748      10   \n",
      "39       4.505596  1.979384e+00         0.005003        0.001415      10   \n",
      "40       0.004000  6.321128e-04         0.000000        0.000000     0.1   \n",
      "41       0.003601  8.003244e-04         0.000000        0.000000     0.1   \n",
      "42       0.040610  2.653377e-03         0.003999        0.000633     0.1   \n",
      "43       0.004201  7.483681e-04         0.000000        0.000000     0.1   \n",
      "44       0.935543  6.640696e-02         0.004801        0.000980     0.1   \n",
      "45       0.121546  7.944167e-03         0.004401        0.000491     0.1   \n",
      "46       0.085634  5.847772e-03         0.004400        0.000489     0.1   \n",
      "47       0.062614  3.006753e-03         0.004801        0.000397     0.1   \n",
      "48       0.924208  5.023182e-02         0.005192        0.000983     0.1   \n",
      "49       1.777576  1.002769e-01         0.004406        0.001014     0.1   \n",
      "50       0.003400  4.897272e-04         0.000000        0.000000     0.1   \n",
      "51       0.003601  4.890865e-04         0.000000        0.000000     0.1   \n",
      "52       0.003198  4.054144e-04         0.000000        0.000000     0.1   \n",
      "53       0.003302  3.989042e-04         0.000000        0.000000     0.1   \n",
      "54       0.003201  4.011648e-04         0.000000        0.000000     0.1   \n",
      "55       0.967491  3.100233e-01         0.005598        0.001363     0.1   \n",
      "56       0.257291  2.635523e-02         0.005201        0.000984     0.1   \n",
      "57       0.003798  3.986393e-04         0.000000        0.000000     0.1   \n",
      "58       4.016929  2.085080e+00         0.005602        0.001021     0.1   \n",
      "59       4.691067  2.157395e+00         0.004801        0.001167     0.1   \n",
      "60       0.003402  4.908180e-04         0.000000        0.000000     100   \n",
      "61       0.003401  4.894153e-04         0.000000        0.000000     100   \n",
      "62       2.195179  1.939514e+00         0.004504        0.000892     100   \n",
      "63       0.004612  1.222231e-03         0.000000        0.000000     100   \n",
      "64       4.600069  1.416507e+00         0.005001        0.000633     100   \n",
      "65       0.186464  1.713251e-02         0.004188        0.000406     100   \n",
      "66       0.195048  1.822164e-02         0.004411        0.000481     100   \n",
      "67       0.101069  9.476199e-03         0.004305        0.000403     100   \n",
      "68       3.732908  1.864189e+00         0.005306        0.000878     100   \n",
      "69       4.698902  2.127317e+00         0.004234        0.000389     100   \n",
      "70       0.003801  4.002391e-04         0.000000        0.000000     100   \n",
      "71       0.004201  3.994009e-04         0.000000        0.000000     100   \n",
      "72       0.003599  4.894550e-04         0.000000        0.000000     100   \n",
      "73       0.003503  6.349390e-04         0.000000        0.000000     100   \n",
      "74       0.003398  4.931761e-04         0.000000        0.000000     100   \n",
      "75       0.816247  2.637650e-01         0.004001        0.000633     100   \n",
      "76       0.212072  2.184508e-02         0.004402        0.000489     100   \n",
      "77       0.003600  8.006235e-04         0.000000        0.000000     100   \n",
      "78       3.766830  1.870896e+00         0.005001        0.001551     100   \n",
      "79       4.209685  1.975130e+00         0.003999        0.000631     100   \n",
      "\n",
      "   param_penalty param_solver  \\\n",
      "0             l1    newton-cg   \n",
      "1             l1        lbfgs   \n",
      "2             l1    liblinear   \n",
      "3             l1          sag   \n",
      "4             l1         saga   \n",
      "5             l2    newton-cg   \n",
      "6             l2        lbfgs   \n",
      "7             l2    liblinear   \n",
      "8             l2          sag   \n",
      "9             l2         saga   \n",
      "10    elasticnet    newton-cg   \n",
      "11    elasticnet        lbfgs   \n",
      "12    elasticnet    liblinear   \n",
      "13    elasticnet          sag   \n",
      "14    elasticnet         saga   \n",
      "15          none    newton-cg   \n",
      "16          none        lbfgs   \n",
      "17          none    liblinear   \n",
      "18          none          sag   \n",
      "19          none         saga   \n",
      "20            l1    newton-cg   \n",
      "21            l1        lbfgs   \n",
      "22            l1    liblinear   \n",
      "23            l1          sag   \n",
      "24            l1         saga   \n",
      "25            l2    newton-cg   \n",
      "26            l2        lbfgs   \n",
      "27            l2    liblinear   \n",
      "28            l2          sag   \n",
      "29            l2         saga   \n",
      "30    elasticnet    newton-cg   \n",
      "31    elasticnet        lbfgs   \n",
      "32    elasticnet    liblinear   \n",
      "33    elasticnet          sag   \n",
      "34    elasticnet         saga   \n",
      "35          none    newton-cg   \n",
      "36          none        lbfgs   \n",
      "37          none    liblinear   \n",
      "38          none          sag   \n",
      "39          none         saga   \n",
      "40            l1    newton-cg   \n",
      "41            l1        lbfgs   \n",
      "42            l1    liblinear   \n",
      "43            l1          sag   \n",
      "44            l1         saga   \n",
      "45            l2    newton-cg   \n",
      "46            l2        lbfgs   \n",
      "47            l2    liblinear   \n",
      "48            l2          sag   \n",
      "49            l2         saga   \n",
      "50    elasticnet    newton-cg   \n",
      "51    elasticnet        lbfgs   \n",
      "52    elasticnet    liblinear   \n",
      "53    elasticnet          sag   \n",
      "54    elasticnet         saga   \n",
      "55          none    newton-cg   \n",
      "56          none        lbfgs   \n",
      "57          none    liblinear   \n",
      "58          none          sag   \n",
      "59          none         saga   \n",
      "60            l1    newton-cg   \n",
      "61            l1        lbfgs   \n",
      "62            l1    liblinear   \n",
      "63            l1          sag   \n",
      "64            l1         saga   \n",
      "65            l2    newton-cg   \n",
      "66            l2        lbfgs   \n",
      "67            l2    liblinear   \n",
      "68            l2          sag   \n",
      "69            l2         saga   \n",
      "70    elasticnet    newton-cg   \n",
      "71    elasticnet        lbfgs   \n",
      "72    elasticnet    liblinear   \n",
      "73    elasticnet          sag   \n",
      "74    elasticnet         saga   \n",
      "75          none    newton-cg   \n",
      "76          none        lbfgs   \n",
      "77          none    liblinear   \n",
      "78          none          sag   \n",
      "79          none         saga   \n",
      "\n",
      "                                                        params  \\\n",
      "0             {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "1                 {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "2             {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "3                   {'C': 1, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "4                  {'C': 1, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "5             {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "6                 {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "7             {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "8                   {'C': 1, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "9                  {'C': 1, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "10    {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "11        {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "12    {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "13          {'C': 1, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "14         {'C': 1, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "15          {'C': 1, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "16              {'C': 1, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "17          {'C': 1, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "18                {'C': 1, 'penalty': 'none', 'solver': 'sag'}   \n",
      "19               {'C': 1, 'penalty': 'none', 'solver': 'saga'}   \n",
      "20           {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "21               {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "22           {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "23                 {'C': 10, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "24                {'C': 10, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "25           {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "26               {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "27           {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "28                 {'C': 10, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "29                {'C': 10, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "30   {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "31       {'C': 10, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "32   {'C': 10, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "33         {'C': 10, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "34        {'C': 10, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "35         {'C': 10, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "36             {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "37         {'C': 10, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "38               {'C': 10, 'penalty': 'none', 'solver': 'sag'}   \n",
      "39              {'C': 10, 'penalty': 'none', 'solver': 'saga'}   \n",
      "40          {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "41              {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "42          {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "43                {'C': 0.1, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "44               {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "45          {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "46              {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "47          {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "48                {'C': 0.1, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "49               {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "50  {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "51      {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "52  {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "53        {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "54       {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "55        {'C': 0.1, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "56            {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "57        {'C': 0.1, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "58              {'C': 0.1, 'penalty': 'none', 'solver': 'sag'}   \n",
      "59             {'C': 0.1, 'penalty': 'none', 'solver': 'saga'}   \n",
      "60          {'C': 100, 'penalty': 'l1', 'solver': 'newton-cg'}   \n",
      "61              {'C': 100, 'penalty': 'l1', 'solver': 'lbfgs'}   \n",
      "62          {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "63                {'C': 100, 'penalty': 'l1', 'solver': 'sag'}   \n",
      "64               {'C': 100, 'penalty': 'l1', 'solver': 'saga'}   \n",
      "65          {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "66              {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}   \n",
      "67          {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}   \n",
      "68                {'C': 100, 'penalty': 'l2', 'solver': 'sag'}   \n",
      "69               {'C': 100, 'penalty': 'l2', 'solver': 'saga'}   \n",
      "70  {'C': 100, 'penalty': 'elasticnet', 'solver': 'newton-cg'}   \n",
      "71      {'C': 100, 'penalty': 'elasticnet', 'solver': 'lbfgs'}   \n",
      "72  {'C': 100, 'penalty': 'elasticnet', 'solver': 'liblinear'}   \n",
      "73        {'C': 100, 'penalty': 'elasticnet', 'solver': 'sag'}   \n",
      "74       {'C': 100, 'penalty': 'elasticnet', 'solver': 'saga'}   \n",
      "75        {'C': 100, 'penalty': 'none', 'solver': 'newton-cg'}   \n",
      "76            {'C': 100, 'penalty': 'none', 'solver': 'lbfgs'}   \n",
      "77        {'C': 100, 'penalty': 'none', 'solver': 'liblinear'}   \n",
      "78              {'C': 100, 'penalty': 'none', 'solver': 'sag'}   \n",
      "79             {'C': 100, 'penalty': 'none', 'solver': 'saga'}   \n",
      "\n",
      "    split0_test_score  split1_test_score  split2_test_score  \\\n",
      "0                 NaN                NaN                NaN   \n",
      "1                 NaN                NaN                NaN   \n",
      "2            0.246066           0.247423           0.269341   \n",
      "3                 NaN                NaN                NaN   \n",
      "4            0.247839           0.249267           0.275072   \n",
      "5            0.244666           0.246334           0.272989   \n",
      "6            0.244666           0.246334           0.272597   \n",
      "7            0.245763           0.248175           0.268571   \n",
      "8            0.244318           0.246696           0.272989   \n",
      "9            0.246809           0.246334           0.272597   \n",
      "10                NaN                NaN                NaN   \n",
      "11                NaN                NaN                NaN   \n",
      "12                NaN                NaN                NaN   \n",
      "13                NaN                NaN                NaN   \n",
      "14                NaN                NaN                NaN   \n",
      "15           0.249645           0.241279           0.271817   \n",
      "16           0.246809           0.241983           0.272206   \n",
      "17                NaN                NaN                NaN   \n",
      "18           0.246809           0.241983           0.272597   \n",
      "19           0.246459           0.242336           0.272597   \n",
      "20                NaN                NaN                NaN   \n",
      "21                NaN                NaN                NaN   \n",
      "22           0.243281           0.242690           0.273381   \n",
      "23                NaN                NaN                NaN   \n",
      "24           0.246110           0.242690           0.272989   \n",
      "25           0.243972           0.239766           0.271817   \n",
      "26           0.243972           0.241983           0.272206   \n",
      "27           0.243972           0.242336           0.271817   \n",
      "28           0.243626           0.241983           0.271817   \n",
      "29           0.242938           0.241983           0.272989   \n",
      "30                NaN                NaN                NaN   \n",
      "31                NaN                NaN                NaN   \n",
      "32                NaN                NaN                NaN   \n",
      "33                NaN                NaN                NaN   \n",
      "34                NaN                NaN                NaN   \n",
      "35           0.249645           0.241279           0.271817   \n",
      "36           0.246809           0.241983           0.272206   \n",
      "37                NaN                NaN                NaN   \n",
      "38           0.246809           0.241983           0.272597   \n",
      "39           0.246459           0.242336           0.272597   \n",
      "40                NaN                NaN                NaN   \n",
      "41                NaN                NaN                NaN   \n",
      "42           0.224719           0.238298           0.251121   \n",
      "43                NaN                NaN                NaN   \n",
      "44           0.231332           0.249624           0.266871   \n",
      "45           0.241379           0.250746           0.267647   \n",
      "46           0.241379           0.250746           0.267647   \n",
      "47           0.242511           0.236842           0.268222   \n",
      "48           0.241379           0.250746           0.268041   \n",
      "49           0.241379           0.250746           0.268041   \n",
      "50                NaN                NaN                NaN   \n",
      "51                NaN                NaN                NaN   \n",
      "52                NaN                NaN                NaN   \n",
      "53                NaN                NaN                NaN   \n",
      "54                NaN                NaN                NaN   \n",
      "55           0.249645           0.241279           0.271817   \n",
      "56           0.246809           0.241983           0.272206   \n",
      "57                NaN                NaN                NaN   \n",
      "58           0.246809           0.241983           0.272597   \n",
      "59           0.246459           0.242336           0.272597   \n",
      "60                NaN                NaN                NaN   \n",
      "61                NaN                NaN                NaN   \n",
      "62           0.246809           0.241983           0.272597   \n",
      "63                NaN                NaN                NaN   \n",
      "64           0.243281           0.242336           0.274170   \n",
      "65           0.246809           0.241983           0.272206   \n",
      "66           0.246809           0.241983           0.272206   \n",
      "67           0.246809           0.241983           0.272206   \n",
      "68           0.246809           0.241983           0.272206   \n",
      "69           0.243972           0.242336           0.272597   \n",
      "70                NaN                NaN                NaN   \n",
      "71                NaN                NaN                NaN   \n",
      "72                NaN                NaN                NaN   \n",
      "73                NaN                NaN                NaN   \n",
      "74                NaN                NaN                NaN   \n",
      "75           0.249645           0.241279           0.271817   \n",
      "76           0.246809           0.241983           0.272206   \n",
      "77                NaN                NaN                NaN   \n",
      "78           0.246809           0.241983           0.272597   \n",
      "79           0.246459           0.242336           0.272597   \n",
      "\n",
      "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
      "0                 NaN                NaN              NaN             NaN   \n",
      "1                 NaN                NaN              NaN             NaN   \n",
      "2            0.266106           0.246772         0.255142        0.010333   \n",
      "3                 NaN                NaN              NaN             NaN   \n",
      "4            0.267229           0.245714         0.257024        0.011852   \n",
      "5            0.266854           0.247482         0.255665        0.011835   \n",
      "6            0.266854           0.247482         0.255587        0.011721   \n",
      "7            0.267229           0.246066         0.255161        0.010444   \n",
      "8            0.266854           0.247126         0.255597        0.011894   \n",
      "9            0.266854           0.247126         0.255944        0.011401   \n",
      "10                NaN                NaN              NaN             NaN   \n",
      "11                NaN                NaN              NaN             NaN   \n",
      "12                NaN                NaN              NaN             NaN   \n",
      "13                NaN                NaN              NaN             NaN   \n",
      "14                NaN                NaN              NaN             NaN   \n",
      "15           0.262794           0.244253         0.253958        0.011581   \n",
      "16           0.262794           0.242511         0.253260        0.012118   \n",
      "17                NaN                NaN              NaN             NaN   \n",
      "18           0.263158           0.242511         0.253411        0.012297   \n",
      "19           0.263158           0.243205         0.253551        0.012150   \n",
      "20                NaN                NaN              NaN             NaN   \n",
      "21                NaN                NaN              NaN             NaN   \n",
      "22           0.267409           0.244253         0.254203        0.013365   \n",
      "23                NaN                NaN              NaN             NaN   \n",
      "24           0.265734           0.243205         0.254146        0.012687   \n",
      "25           0.263889           0.242511         0.252391        0.012942   \n",
      "26           0.263889           0.242511         0.252912        0.012652   \n",
      "27           0.263889           0.242511         0.252905        0.012473   \n",
      "28           0.264256           0.242857         0.252908        0.012593   \n",
      "29           0.264993           0.243205         0.253221        0.013128   \n",
      "30                NaN                NaN              NaN             NaN   \n",
      "31                NaN                NaN              NaN             NaN   \n",
      "32                NaN                NaN              NaN             NaN   \n",
      "33                NaN                NaN              NaN             NaN   \n",
      "34                NaN                NaN              NaN             NaN   \n",
      "35           0.262794           0.244253         0.253958        0.011581   \n",
      "36           0.262794           0.242511         0.253260        0.012118   \n",
      "37                NaN                NaN              NaN             NaN   \n",
      "38           0.263158           0.242511         0.253411        0.012297   \n",
      "39           0.263158           0.243205         0.253551        0.012150   \n",
      "40                NaN                NaN              NaN             NaN   \n",
      "41                NaN                NaN              NaN             NaN   \n",
      "42           0.263587           0.239884         0.243522        0.013072   \n",
      "43                NaN                NaN              NaN             NaN   \n",
      "44           0.272597           0.237037         0.251492        0.016129   \n",
      "45           0.273239           0.246020         0.255806        0.012439   \n",
      "46           0.273239           0.246020         0.255806        0.012439   \n",
      "47           0.269819           0.246772         0.252833        0.013597   \n",
      "48           0.273625           0.246020         0.255962        0.012622   \n",
      "49           0.273239           0.246377         0.255957        0.012459   \n",
      "50                NaN                NaN              NaN             NaN   \n",
      "51                NaN                NaN              NaN             NaN   \n",
      "52                NaN                NaN              NaN             NaN   \n",
      "53                NaN                NaN              NaN             NaN   \n",
      "54                NaN                NaN              NaN             NaN   \n",
      "55           0.262794           0.244253         0.253958        0.011581   \n",
      "56           0.262794           0.242511         0.253260        0.012118   \n",
      "57                NaN                NaN              NaN             NaN   \n",
      "58           0.263158           0.242511         0.253411        0.012297   \n",
      "59           0.263158           0.243205         0.253551        0.012150   \n",
      "60                NaN                NaN              NaN             NaN   \n",
      "61                NaN                NaN              NaN             NaN   \n",
      "62           0.262794           0.242511         0.253338        0.012240   \n",
      "63                NaN                NaN              NaN             NaN   \n",
      "64           0.263523           0.243205         0.253303        0.013135   \n",
      "65           0.262794           0.242511         0.253260        0.012118   \n",
      "66           0.262794           0.242511         0.253260        0.012118   \n",
      "67           0.262794           0.242511         0.253260        0.012118   \n",
      "68           0.263158           0.242511         0.253333        0.012176   \n",
      "69           0.263523           0.243205         0.253126        0.012537   \n",
      "70                NaN                NaN              NaN             NaN   \n",
      "71                NaN                NaN              NaN             NaN   \n",
      "72                NaN                NaN              NaN             NaN   \n",
      "73                NaN                NaN              NaN             NaN   \n",
      "74                NaN                NaN              NaN             NaN   \n",
      "75           0.262794           0.244253         0.253958        0.011581   \n",
      "76           0.262794           0.242511         0.253260        0.012118   \n",
      "77                NaN                NaN              NaN             NaN   \n",
      "78           0.263158           0.242511         0.253411        0.012297   \n",
      "79           0.263158           0.243205         0.253551        0.012150   \n",
      "\n",
      "    rank_test_score  \n",
      "0                80  \n",
      "1                49  \n",
      "2                11  \n",
      "3                46  \n",
      "4                 1  \n",
      "5                 7  \n",
      "6                 9  \n",
      "7                10  \n",
      "8                 8  \n",
      "9                 4  \n",
      "10               79  \n",
      "11               58  \n",
      "12               74  \n",
      "13               73  \n",
      "14               72  \n",
      "15               14  \n",
      "16               29  \n",
      "17               62  \n",
      "18               22  \n",
      "19               18  \n",
      "20               57  \n",
      "21               60  \n",
      "22               12  \n",
      "23               61  \n",
      "24               13  \n",
      "25               42  \n",
      "26               38  \n",
      "27               40  \n",
      "28               39  \n",
      "29               36  \n",
      "30               63  \n",
      "31               64  \n",
      "32               65  \n",
      "33               66  \n",
      "34               67  \n",
      "35               14  \n",
      "36               29  \n",
      "37               68  \n",
      "38               22  \n",
      "39               18  \n",
      "40               69  \n",
      "41               70  \n",
      "42               44  \n",
      "43               71  \n",
      "44               43  \n",
      "45                5  \n",
      "46                5  \n",
      "47               41  \n",
      "48                2  \n",
      "49                3  \n",
      "50               75  \n",
      "51               76  \n",
      "52               77  \n",
      "53               78  \n",
      "54               59  \n",
      "55               14  \n",
      "56               29  \n",
      "57               53  \n",
      "58               22  \n",
      "59               18  \n",
      "60               47  \n",
      "61               48  \n",
      "62               26  \n",
      "63               56  \n",
      "64               28  \n",
      "65               29  \n",
      "66               29  \n",
      "67               29  \n",
      "68               27  \n",
      "69               37  \n",
      "70               50  \n",
      "71               51  \n",
      "72               55  \n",
      "73               54  \n",
      "74               52  \n",
      "75               14  \n",
      "76               29  \n",
      "77               45  \n",
      "78               22  \n",
      "79               18  \n"
     ]
    }
   ],
   "source": [
    "print(GS.best_params_)\n",
    "print(pd.DataFrame(GS.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        train                        val\n",
      "confusion matrix   [[4894, 2286], [266, 452]]  [[3669, 1761], [61, 100]]\n",
      "acc                                   0.67688                   0.674119\n",
      "precision                            0.165084                   0.053735\n",
      "f1_score                             0.261574                   0.098912\n",
      "recall                               0.629526                   0.621118\n",
      "matthews_corrcoef                     0.18795                   0.105332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "best_parameters = {\n",
    "    'C': 1, 'penalty': 'l1', 'solver': 'saga'\n",
    "}\n",
    "\n",
    "clf = LogisticRegression(**best_parameters,class_weight={1:1,0:0.1},max_iter=10000) \n",
    "\n",
    "clf.fit(train_X, train_y['outcome'])\n",
    "result = clf.predict(val_X)\n",
    "\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'train':tools.get_performance(train_y,clf.predict(train_X)),\n",
    "    'val':tools.get_performance(val_y,result)\n",
    "    }\n",
    "    ))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "758b741b386b57519efd53b073ac35bdb1f696dd4ad70fef9c572829f656d496"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
